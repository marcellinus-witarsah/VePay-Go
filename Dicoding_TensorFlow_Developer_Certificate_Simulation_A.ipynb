{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dicoding_TensorFlow_Developer_Certificate_Simulation_A.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNtIgb32j/GbW4C2W3W9ZNA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marcellinus-witarsah/VePay-Go/blob/main/Dicoding_TensorFlow_Developer_Certificate_Simulation_A.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dicoding TensorFlow Developer Certificate Simulation A"
      ],
      "metadata": {
        "id": "XYbYrb6AA0M3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A1"
      ],
      "metadata": {
        "id": "t_G5UytpBDgB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =================================================================================\n",
        "# PROBLEM A1\n",
        "#\n",
        "# Given two arrays, train a neural network model to match the X to the Y.\n",
        "# Predict the model with new values of X [-2.0, 10.0]\n",
        "# We provide the model prediction, do not change the code.\n",
        "#\n",
        "# The test infrastructure expects a trained model that accepts\n",
        "# an input shape of [1].\n",
        "# Do not use lambda layers in your model.\n",
        "#\n",
        "# Please be aware that this is a linear model.\n",
        "# We will test your model with values in a range as defined in the array to make sure your model is linear.\n",
        "#\n",
        "# Desired loss (MSE) < 1e-4\n",
        "# =================================================================================\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "\n",
        "def solution_A1():\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    X = np.array([-4.0, -3.0, -2.0, -1.0, 0.0, 1.0,\n",
        "                 2.0, 3.0, 4.0, 5.0], dtype=float)\n",
        "    Y = np.array([5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0,\n",
        "                 12.0, 13.0, 14.0, ], dtype=float)\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    model = tf.keras.Sequential(\n",
        "        [\n",
        "         tf.keras.layers.Dense(units=1, input_shape=[1])\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        loss=tf.keras.losses.MeanSquaredError(),\n",
        "        optimizer=tf.keras.optimizers.SGD(learning_rate=0.1)\n",
        "    )\n",
        "\n",
        "    model.fit(\n",
        "        x=X,\n",
        "        y=Y,\n",
        "        epochs=100\n",
        "    )\n",
        "\n",
        "    print(model.predict([-2.0, 10.0]))\n",
        "    return model\n",
        "\n",
        "\n",
        "# The code below is to save your model as a .h5 file.\n",
        "# It will be saved automatically in your Submission folder.\n",
        "if __name__ == '__main__':\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    model = solution_A1()\n",
        "    model.save(\"model_A1.h5\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWoO3kyrBPkD",
        "outputId": "b55b8941-358a-4fd7-8355-f7d65b64011c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 134.8279\n",
            "Epoch 2/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 78.7705\n",
            "Epoch 3/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 46.7805\n",
            "Epoch 4/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 28.2050\n",
            "Epoch 5/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 17.2368\n",
            "Epoch 6/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 10.6585\n",
            "Epoch 7/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 6.6570\n",
            "Epoch 8/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 4.1927\n",
            "Epoch 9/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.6587\n",
            "Epoch 10/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.6953\n",
            "Epoch 11/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.0858\n",
            "Epoch 12/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.6978\n",
            "Epoch 13/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.4498\n",
            "Epoch 14/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.2905\n",
            "Epoch 15/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.1879\n",
            "Epoch 16/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.1217\n",
            "Epoch 17/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0789\n",
            "Epoch 18/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0512\n",
            "Epoch 19/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0333\n",
            "Epoch 20/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0216\n",
            "Epoch 21/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0140\n",
            "Epoch 22/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0091\n",
            "Epoch 23/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0059\n",
            "Epoch 24/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0039\n",
            "Epoch 25/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0025\n",
            "Epoch 26/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0016\n",
            "Epoch 27/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0011\n",
            "Epoch 28/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 6.9124e-04\n",
            "Epoch 29/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.4971e-04\n",
            "Epoch 30/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.9258e-04\n",
            "Epoch 31/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.9034e-04\n",
            "Epoch 32/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.2383e-04\n",
            "Epoch 33/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 8.0567e-05\n",
            "Epoch 34/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 5.2424e-05\n",
            "Epoch 35/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 3.4112e-05\n",
            "Epoch 36/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 2.2196e-05\n",
            "Epoch 37/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1.4440e-05\n",
            "Epoch 38/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 9.3946e-06\n",
            "Epoch 39/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.1124e-06\n",
            "Epoch 40/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 3.9780e-06\n",
            "Epoch 41/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.5880e-06\n",
            "Epoch 42/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 1.6849e-06\n",
            "Epoch 43/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.0958e-06\n",
            "Epoch 44/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 7.1237e-07\n",
            "Epoch 45/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.6285e-07\n",
            "Epoch 46/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.0121e-07\n",
            "Epoch 47/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1.9591e-07\n",
            "Epoch 48/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1.2754e-07\n",
            "Epoch 49/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 8.3172e-08\n",
            "Epoch 50/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 5.4339e-08\n",
            "Epoch 51/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 3.5351e-08\n",
            "Epoch 52/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 2.2900e-08\n",
            "Epoch 53/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.4925e-08\n",
            "Epoch 54/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 9.7375e-09\n",
            "Epoch 55/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.3688e-09\n",
            "Epoch 56/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.1993e-09\n",
            "Epoch 57/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 2.7632e-09\n",
            "Epoch 58/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1.7885e-09\n",
            "Epoch 59/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1.1449e-09\n",
            "Epoch 60/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 7.4829e-10\n",
            "Epoch 61/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.6434e-10\n",
            "Epoch 62/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 3.2028e-10\n",
            "Epoch 63/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.0023e-10\n",
            "Epoch 64/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.2571e-10\n",
            "Epoch 65/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 8.8266e-11\n",
            "Epoch 66/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 5.6616e-11\n",
            "Epoch 67/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 3.1878e-11\n",
            "Epoch 68/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.1942e-11\n",
            "Epoch 69/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.4666e-11\n",
            "Epoch 70/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 7.9353e-12\n",
            "Epoch 71/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 4.0473e-12\n",
            "Epoch 72/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 3.9336e-12\n",
            "Epoch 73/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 3.6380e-12\n",
            "Epoch 74/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 3.9336e-12\n",
            "Epoch 75/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 3.6380e-12\n",
            "Epoch 76/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 3.9336e-12\n",
            "Epoch 77/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 3.6380e-12\n",
            "Epoch 78/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 3.9336e-12\n",
            "Epoch 79/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 3.6380e-12\n",
            "Epoch 80/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 3.9336e-12\n",
            "Epoch 81/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 3.6380e-12\n",
            "Epoch 82/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 3.9336e-12\n",
            "Epoch 83/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 3.6380e-12\n",
            "Epoch 84/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.9336e-12\n",
            "Epoch 85/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 3.6380e-12\n",
            "Epoch 86/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 3.9336e-12\n",
            "Epoch 87/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 3.6380e-12\n",
            "Epoch 88/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 3.9336e-12\n",
            "Epoch 89/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 3.6380e-12\n",
            "Epoch 90/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.9336e-12\n",
            "Epoch 91/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.6380e-12\n",
            "Epoch 92/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 3.9336e-12\n",
            "Epoch 93/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.6380e-12\n",
            "Epoch 94/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.9336e-12\n",
            "Epoch 95/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.6380e-12\n",
            "Epoch 96/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 3.9336e-12\n",
            "Epoch 97/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.6380e-12\n",
            "Epoch 98/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 3.9336e-12\n",
            "Epoch 99/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 3.6380e-12\n",
            "Epoch 100/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.9336e-12\n",
            "[[ 6.999998]\n",
            " [18.999998]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A2"
      ],
      "metadata": {
        "id": "gX-ss4YMBIJl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================================================\n",
        "# PROBLEM A2\n",
        "#\n",
        "# Build a Neural Network Model for Horse or Human Dataset.\n",
        "# The test will expect it to classify binary classes.\n",
        "# Your input layer should accept 150x150 with 3 bytes color as the input shape.\n",
        "# Don't use lambda layers in your model.\n",
        "#\n",
        "# The dataset used in this problem is created by Laurence Moroney (laurencemoroney.com).\n",
        "#\n",
        "# Desired accuracy and validation_accuracy > 83%\n",
        "# ======================================================================================\n",
        "\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from keras_preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "\n",
        "def solution_A2():\n",
        "    data_url_1 = 'https://github.com/dicodingacademy/assets/releases/download/release-horse-or-human/horse-or-human.zip'\n",
        "    urllib.request.urlretrieve(data_url_1, 'horse-or-human-a2.zip')\n",
        "    local_file = 'horse-or-human-a2.zip'\n",
        "    zip_ref = zipfile.ZipFile(local_file, 'r')\n",
        "    zip_ref.extractall('data/horse-or-human-a2')\n",
        "\n",
        "    data_url_2 = 'https://github.com/dicodingacademy/assets/raw/main/Simulation/machine_learning/validation-horse-or-human.zip'\n",
        "    urllib.request.urlretrieve(data_url_2, 'validation-horse-or-human-a2.zip')\n",
        "    local_file = 'validation-horse-or-human-a2.zip'\n",
        "    zip_ref = zipfile.ZipFile(local_file, 'r')\n",
        "    zip_ref.extractall('data/validation-horse-or-human-a2')\n",
        "    zip_ref.close()\n",
        "\n",
        "    TRAINING_DIR = 'data/horse-or-human-a2'\n",
        "    VALID_DIR = 'data/validation-horse-or-human-a2'\n",
        "    train_datagen = ImageDataGenerator(\n",
        "        rescale = 1/255,\n",
        "        rotation_range=10,\n",
        "        height_shift_range=0.1,\n",
        "        width_shift_range=0.1,\n",
        "        zoom_range=0.1,\n",
        "        shear_range=0.1,\n",
        "        horizontal_flip=True,\n",
        "    )\n",
        "\n",
        "    valid_datagen = ImageDataGenerator(\n",
        "        rescale = 1/255,\n",
        "    )\n",
        "\n",
        "    TARGET_SIZE = (150, 150)\n",
        "    BATCH_SIZE = 32\n",
        "    SEED=42\n",
        "\n",
        "    # YOUR IMAGE SIZE SHOULD BE 150x150\n",
        "    train_generator=train_datagen.flow_from_directory(\n",
        "        directory=TRAINING_DIR,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        target_size=TARGET_SIZE,\n",
        "        class_mode='binary',\n",
        "        seed=SEED,\n",
        "    )\n",
        "    \n",
        "    valid_generator=valid_datagen.flow_from_directory(\n",
        "        directory=VALID_DIR,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        target_size=TARGET_SIZE,\n",
        "        class_mode='binary',\n",
        "        seed=SEED,\n",
        "    )\n",
        "\n",
        "    INPUT_SHAPE = TARGET_SIZE + (3,)\n",
        "    model=tf.keras.models.Sequential([\n",
        "        # YOUR CODE HERE, end with a Neuron Dense, activated by sigmoid\n",
        "        tf.keras.layers.Conv2D(\n",
        "            filters=16,\n",
        "            input_shape=INPUT_SHAPE,\n",
        "            kernel_size=(3, 3),\n",
        "            activation=tf.nn.relu,\n",
        "            # padding='same',\n",
        "            kernel_initializer=tf.keras.initializers.he_normal,\n",
        "        ),\n",
        "        tf.keras.layers.MaxPool2D(\n",
        "            pool_size=(2, 2),\n",
        "            # padding='same',\n",
        "        ),\n",
        "        tf.keras.layers.Conv2D(\n",
        "            filters=32,\n",
        "            kernel_size=(3, 3),\n",
        "            activation=tf.nn.relu,\n",
        "            # padding='same',\n",
        "            kernel_initializer=tf.keras.initializers.he_normal,\n",
        "        ),\n",
        "        tf.keras.layers.Conv2D(\n",
        "            filters=32,\n",
        "            kernel_size=(3, 3),\n",
        "            activation=tf.nn.relu,\n",
        "            # padding='same',\n",
        "            kernel_initializer=tf.keras.initializers.he_normal,\n",
        "        ),\n",
        "        tf.keras.layers.MaxPool2D(\n",
        "            pool_size=(2, 2),\n",
        "            # padding='same',\n",
        "        ),\n",
        "        tf.keras.layers.Conv2D(\n",
        "            filters=64,\n",
        "            kernel_size=(3, 3),\n",
        "            activation=tf.nn.relu,\n",
        "            # padding='same',\n",
        "            kernel_initializer=tf.keras.initializers.he_normal,\n",
        "        ),\n",
        "        tf.keras.layers.Conv2D(\n",
        "            filters=64,\n",
        "            kernel_size=(3, 3),\n",
        "            activation=tf.nn.relu,\n",
        "            # padding='same',\n",
        "            kernel_initializer=tf.keras.initializers.he_normal,\n",
        "        ),\n",
        "        tf.keras.layers.MaxPool2D(\n",
        "            pool_size=(2, 2),\n",
        "            # padding='same',\n",
        "        ),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(\n",
        "            units=512, \n",
        "            activation=tf.nn.relu, \n",
        "            kernel_initializer=tf.keras.initializers.he_normal,\n",
        "        ),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "\n",
        "    model.compile(\n",
        "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "        optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.001),\n",
        "        metrics = [tf.keras.metrics.BinaryAccuracy()]\n",
        "    )\n",
        "\n",
        "    model.fit(\n",
        "        train_generator,\n",
        "        epochs=10,\n",
        "        validation_data=valid_generator,\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# The code below is to save your model as a .h5 file.\n",
        "# It will be saved automatically in your Submission folder.\n",
        "if __name__ == '__main__':\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    model=solution_A2()\n",
        "    model.save(\"model_A2.h5\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBRUv0OLCp95",
        "outputId": "e254fd80-6115-4df3-9988-88706a3c22cf"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1027 images belonging to 2 classes.\n",
            "Found 256 images belonging to 2 classes.\n",
            "Epoch 1/10\n",
            "33/33 [==============================] - 12s 332ms/step - loss: 0.8414 - binary_accuracy: 0.6709 - val_loss: 1.1332 - val_binary_accuracy: 0.8047\n",
            "Epoch 2/10\n",
            "33/33 [==============================] - 11s 330ms/step - loss: 0.2952 - binary_accuracy: 0.8685 - val_loss: 2.0661 - val_binary_accuracy: 0.7305\n",
            "Epoch 3/10\n",
            "33/33 [==============================] - 11s 326ms/step - loss: 0.1882 - binary_accuracy: 0.9367 - val_loss: 4.1074 - val_binary_accuracy: 0.6914\n",
            "Epoch 4/10\n",
            "33/33 [==============================] - 12s 353ms/step - loss: 0.1684 - binary_accuracy: 0.9494 - val_loss: 3.0320 - val_binary_accuracy: 0.7148\n",
            "Epoch 5/10\n",
            "33/33 [==============================] - 11s 326ms/step - loss: 0.1930 - binary_accuracy: 0.9494 - val_loss: 3.6817 - val_binary_accuracy: 0.6758\n",
            "Epoch 6/10\n",
            "33/33 [==============================] - 11s 325ms/step - loss: 0.1349 - binary_accuracy: 0.9591 - val_loss: 4.2417 - val_binary_accuracy: 0.7070\n",
            "Epoch 7/10\n",
            "33/33 [==============================] - 11s 327ms/step - loss: 0.0547 - binary_accuracy: 0.9786 - val_loss: 5.5206 - val_binary_accuracy: 0.6562\n",
            "Epoch 8/10\n",
            "33/33 [==============================] - 11s 329ms/step - loss: 0.1075 - binary_accuracy: 0.9649 - val_loss: 3.6724 - val_binary_accuracy: 0.7539\n",
            "Epoch 9/10\n",
            "33/33 [==============================] - 11s 323ms/step - loss: 0.0668 - binary_accuracy: 0.9786 - val_loss: 7.9219 - val_binary_accuracy: 0.7070\n",
            "Epoch 10/10\n",
            "33/33 [==============================] - 11s 320ms/step - loss: 0.0388 - binary_accuracy: 0.9873 - val_loss: 7.6070 - val_binary_accuracy: 0.6797\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A3"
      ],
      "metadata": {
        "id": "aG18zjiDBJQc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================================================================\n",
        "# PROBLEM A3\n",
        "#\n",
        "# Build a classifier for the Human or Horse Dataset with Transfer Learning.\n",
        "# The test will expect it to classify binary classes.\n",
        "# Note that all the layers in the pre-trained model are non-trainable.\n",
        "# Do not use lambda layers in your model.\n",
        "#\n",
        "# The horse-or-human dataset used in this problem is created by Laurence Moroney (laurencemoroney.com).\n",
        "# Inception_v3, pre-trained model used in this problem is developed by Google.\n",
        "#\n",
        "# Desired accuracy and validation_accuracy > 97%.\n",
        "# =======================================================================================================\n",
        "\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import tensorflow as tf\n",
        "from keras_preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "\n",
        "\n",
        "def solution_A3():\n",
        "    TARGET_SHAPE = (150, 150)\n",
        "    INPUT_SHAPE = (150, 150) + (3,)\n",
        "    inceptionv3 = 'https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
        "    urllib.request.urlretrieve(\n",
        "        inceptionv3, 'inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5')\n",
        "    local_weights_file = 'inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
        "\n",
        "    pre_trained_model =  tf.keras.applications.inception_v3.InceptionV3(\n",
        "        include_top = False,\n",
        "        weights=None,\n",
        "        input_shape = INPUT_SHAPE\n",
        "    )\n",
        "\n",
        "    for layer in pre_trained_model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    last_layer =  pre_trained_model.get_layer('mixed8')\n",
        "    last_output = last_layer.output\n",
        "\n",
        "    data_url_1 = 'https://github.com/dicodingacademy/assets/releases/download/release-horse-or-human/horse-or-human.zip'\n",
        "    urllib.request.urlretrieve(data_url_1, 'horse-or-human-a3.zip')\n",
        "    local_file = 'horse-or-human-a3.zip'\n",
        "    zip_ref = zipfile.ZipFile(local_file, 'r')\n",
        "    zip_ref.extractall('data1/horse-or-human-a3')\n",
        "    zip_ref.close()\n",
        "\n",
        "    data_url_2 = 'https://github.com/dicodingacademy/assets/raw/main/Simulation/machine_learning/validation-horse-or-human.zip'\n",
        "    urllib.request.urlretrieve(data_url_2, 'validation-horse-or-human-a3.zip')\n",
        "    local_file = 'validation-horse-or-human-a3.zip'\n",
        "    zip_ref = zipfile.ZipFile(local_file, 'r')\n",
        "    zip_ref.extractall('data/validation-horse-or-human-a3')\n",
        "    zip_ref.close()\n",
        "\n",
        "    train_dir = 'data/horse-or-human-a3'\n",
        "    validation_dir = 'data/validation-horse-or-human-a3'\n",
        "\n",
        "    train_datagen = ImageDataGenerator(\n",
        "        rescale=1. / 255,\n",
        "        rotation_range=40,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest'\n",
        "    )\n",
        "\n",
        "    validation_datagen = ImageDataGenerator(\n",
        "        rescale=1./255\n",
        "    )\n",
        "\n",
        "    train_generator = train_datagen.flow_from_directory(\n",
        "        directory=train_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=20,\n",
        "        class_mode='binary',\n",
        "        shuffle=True,\n",
        "        seed=42,\n",
        "    )\n",
        "\n",
        "    validation_generator = validation_datagen.flow_from_directory(\n",
        "        directory=validation_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=20,\n",
        "        class_mode='binary',\n",
        "        shuffle=True,\n",
        "        seed=42\n",
        "    )\n",
        "\n",
        "\n",
        "    x = tf.keras.layers.Flatten()(last_output)\n",
        "    x = tf.keras.layers.Dense(\n",
        "        units=1024, \n",
        "        activation=tf.nn.relu, \n",
        "        kernel_initializer=tf.keras.initializers.he_normal,\n",
        "        )(x)\n",
        "    x = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = Model(pre_trained_model.input, x)\n",
        "\n",
        "    model.compile(optimizer=RMSprop(lr=0.0001),\n",
        "                  loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "                  metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
        "    \n",
        "    model.fit(\n",
        "        train_generator,\n",
        "        epochs=10,\n",
        "        validation_data=validation_generator,\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# The code below is to save your model as a .h5 file.\n",
        "# It will be saved automatically in your Submission folder.\n",
        "if __name__ == '__main__':\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    model=solution_A3()\n",
        "    model.save(\"model_A3.h5\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "nNSB2pyfHgpU",
        "outputId": "e1a2a1d5-64ce-4fe7-ae43-5ff7c3afdbed"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-f91c43e07778>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;31m# DO NOT CHANGE THIS CODE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msolution_A3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model_A3.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-f91c43e07778>\u001b[0m in \u001b[0;36msolution_A3\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mclass_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m     )\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras_preprocessing/image/image_data_generator.py\u001b[0m in \u001b[0;36mflow_from_directory\u001b[0;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation)\u001b[0m\n\u001b[1;32m    541\u001b[0m             \u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m             \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    544\u001b[0m         )\n\u001b[1;32m    545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras_preprocessing/image/directory_iterator.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, dtype)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0msubdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                     \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/horse-or-human-a3'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A4"
      ],
      "metadata": {
        "id": "tkK7tPpWBJTo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================================================================\n",
        "# PROBLEM A4\n",
        "#\n",
        "# Build and train a binary classifier for the IMDB review dataset.\n",
        "# The classifier should have a final layer with 1 neuron activated by sigmoid.\n",
        "# Do not use lambda layers in your model.\n",
        "#\n",
        "# The dataset used in this problem is originally published in http://ai.stanford.edu/~amaas/data/sentiment/\n",
        "#\n",
        "# Desired accuracy and validation_accuracy > 83%\n",
        "# ===========================================================================================================\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "def solution_A4():\n",
        "    imdb, info = tfds.load(\"imdb_reviews\", with_info=True, as_supervised=True)\n",
        "    # YOUR CODE HERE\n",
        "    train_data = imdb['train']\n",
        "    test_data = imdb['test']\n",
        "\n",
        "    training_sentences = []\n",
        "    training_labels = []\n",
        "    testing_sentences = []\n",
        "    testing_labels = []\n",
        "\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    for s, l in train_data:\n",
        "        training_sentences.append(s.numpy().decode('utf8'))\n",
        "        training_labels.append(l.numpy())\n",
        "\n",
        "    for s, l in test_data:\n",
        "        testing_sentences.append(s.numpy().decode('utf8'))\n",
        "        testing_labels.append(l.numpy())\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    # Make sure you used all of these parameters or test may fail\n",
        "    vocab_size = 10000\n",
        "    embedding_dim = 16\n",
        "    max_length = 120\n",
        "    trunc_type = 'post'\n",
        "    oov_tok = \"<OOV>\"\n",
        "\n",
        "    # Fit your tokenizer with training data\n",
        "    tokenizer =  # YOUR CODE HERE\n",
        "\n",
        "    model = tf.keras.Sequential([\n",
        "        # YOUR CODE HERE. Do not change the last layer.\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# The code below is to save your model as a .h5 file.\n",
        "# It will be saved automatically in your Submission folder.\n",
        "if __name__ == '__main__':\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    model = solution_A4()\n",
        "    model.save(\"model_A4.h5\")\n"
      ],
      "metadata": {
        "id": "pRzEZjUASYTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A5"
      ],
      "metadata": {
        "id": "K8qltAESBJWm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =======================================================================================\n",
        "# PROBLEM A5\n",
        "#\n",
        "# Build and train a neural network model using the Sunspots.csv dataset.\n",
        "# Use MAE as the metrics of your neural network model.\n",
        "# We provided code for normalizing the data. Please do not change the code.\n",
        "# Do not use lambda layers in your model.\n",
        "#\n",
        "# The dataset used in this problem is downloaded from kaggle.com/robervalt/sunspots\n",
        "#\n",
        "# Desired MAE < 0.15 on the normalized dataset.\n",
        "# ========================================================================================\n",
        "\n",
        "import csv\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import urllib\n",
        "\n",
        "# DO NOT CHANGE THIS CODE\n",
        "def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n",
        "    series = tf.expand_dims(series, axis=-1)\n",
        "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)\n",
        "    ds = ds.flat_map(lambda w: w.batch(window_size + 1))\n",
        "    ds = ds.shuffle(shuffle_buffer)\n",
        "    ds = ds.map(lambda w: (w[:-1], w[1:]))\n",
        "    return ds.batch(batch_size).prefetch(1)\n",
        "\n",
        "\n",
        "def solution_A5():\n",
        "    data_url = 'https://github.com/dicodingacademy/assets/raw/main/Simulation/machine_learning/sunspots.csv'\n",
        "    urllib.request.urlretrieve(data_url, 'sunspots.csv')\n",
        "\n",
        "    time_step = []\n",
        "    sunspots = []\n",
        "\n",
        "    with open('sunspots.csv') as csvfile:\n",
        "      reader = csv.reader(csvfile, delimiter=',')\n",
        "      next(reader)\n",
        "      for row in reader:\n",
        "        sunspots.append(  # YOUR CODE HERE)\n",
        "        time_step.append(  # YOUR CODE HERE)\n",
        "\n",
        "    series=  # YOUR CODE HERE\n",
        "\n",
        "    # Normalization Function. DO NOT CHANGE THIS CODE\n",
        "    min=np.min(series)\n",
        "    max=np.max(series)\n",
        "    series -= min\n",
        "    series /= max\n",
        "    time=np.array(time_step)\n",
        "\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    split_time=3000\n",
        "\n",
        "\n",
        "    time_train=  # YOUR CODE HERE\n",
        "    x_train=  # YOUR CODE HERE\n",
        "    time_valid=  # YOUR CODE HERE\n",
        "    x_valid=  # YOUR CODE HERE\n",
        "\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    window_size=30\n",
        "    batch_size=32\n",
        "    shuffle_buffer_size=1000\n",
        "\n",
        "\n",
        "    train_set=windowed_dataset(x_train, window_size=window_size,\n",
        "                               batch_size=batch_size, shuffle_buffer=shuffle_buffer_size)\n",
        "\n",
        "\n",
        "    model=tf.keras.models.Sequential([\n",
        "      # YOUR CODE HERE.\n",
        "      tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "    # YOUR CODE\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "# The code below is to save your model as a .h5 file.\n",
        "# It will be saved automatically in your Submission folder.\n",
        "if __name__ == '__main__':\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    model=solution_A5()\n",
        "    model.save(\"model_A5.h5\")\n"
      ],
      "metadata": {
        "id": "RlqnEKYlBJY-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dicoding_TensorFlow_Developer_Certificate_Simulation_A.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMegJ6dc5Nl3XA8ALiApg8Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marcellinus-witarsah/VePay-Go/blob/main/Dicoding_TensorFlow_Developer_Certificate_Simulation_A.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dicoding TensorFlow Developer Certificate Simulation A"
      ],
      "metadata": {
        "id": "XYbYrb6AA0M3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A1"
      ],
      "metadata": {
        "id": "t_G5UytpBDgB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =================================================================================\n",
        "# PROBLEM A1\n",
        "#\n",
        "# Given two arrays, train a neural network model to match the X to the Y.\n",
        "# Predict the model with new values of X [-2.0, 10.0]\n",
        "# We provide the model prediction, do not change the code.\n",
        "#\n",
        "# The test infrastructure expects a trained model that accepts\n",
        "# an input shape of [1].\n",
        "# Do not use lambda layers in your model.\n",
        "#\n",
        "# Please be aware that this is a linear model.\n",
        "# We will test your model with values in a range as defined in the array to make sure your model is linear.\n",
        "#\n",
        "# Desired loss (MSE) < 1e-4\n",
        "# =================================================================================\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "\n",
        "def solution_A1():\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    X = np.array([-4.0, -3.0, -2.0, -1.0, 0.0, 1.0,\n",
        "                 2.0, 3.0, 4.0, 5.0], dtype=float)\n",
        "    Y = np.array([5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0,\n",
        "                 12.0, 13.0, 14.0, ], dtype=float)\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    model = tf.keras.Sequential(\n",
        "        [\n",
        "         tf.keras.layers.Dense(units=1, input_shape=[1])\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        loss=tf.keras.losses.MeanSquaredError(),\n",
        "        optimizer=tf.keras.optimizers.SGD(learning_rate=0.1)\n",
        "    )\n",
        "\n",
        "    model.fit(\n",
        "        x=X,\n",
        "        y=Y,\n",
        "        epochs=100\n",
        "    )\n",
        "\n",
        "    print(model.predict([-2.0, 10.0]))\n",
        "    return model\n",
        "\n",
        "\n",
        "# The code below is to save your model as a .h5 file.\n",
        "# It will be saved automatically in your Submission folder.\n",
        "if __name__ == '__main__':\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    model = solution_A1()\n",
        "    model.save(\"model_A1.h5\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWoO3kyrBPkD",
        "outputId": "b55b8941-358a-4fd7-8355-f7d65b64011c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 134.8279\n",
            "Epoch 2/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 78.7705\n",
            "Epoch 3/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 46.7805\n",
            "Epoch 4/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 28.2050\n",
            "Epoch 5/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 17.2368\n",
            "Epoch 6/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 10.6585\n",
            "Epoch 7/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 6.6570\n",
            "Epoch 8/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 4.1927\n",
            "Epoch 9/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.6587\n",
            "Epoch 10/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.6953\n",
            "Epoch 11/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.0858\n",
            "Epoch 12/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.6978\n",
            "Epoch 13/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.4498\n",
            "Epoch 14/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.2905\n",
            "Epoch 15/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.1879\n",
            "Epoch 16/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.1217\n",
            "Epoch 17/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0789\n",
            "Epoch 18/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0512\n",
            "Epoch 19/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0333\n",
            "Epoch 20/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0216\n",
            "Epoch 21/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0140\n",
            "Epoch 22/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0091\n",
            "Epoch 23/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0059\n",
            "Epoch 24/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0039\n",
            "Epoch 25/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0025\n",
            "Epoch 26/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0016\n",
            "Epoch 27/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0011\n",
            "Epoch 28/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 6.9124e-04\n",
            "Epoch 29/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.4971e-04\n",
            "Epoch 30/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.9258e-04\n",
            "Epoch 31/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.9034e-04\n",
            "Epoch 32/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.2383e-04\n",
            "Epoch 33/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 8.0567e-05\n",
            "Epoch 34/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 5.2424e-05\n",
            "Epoch 35/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 3.4112e-05\n",
            "Epoch 36/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 2.2196e-05\n",
            "Epoch 37/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1.4440e-05\n",
            "Epoch 38/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 9.3946e-06\n",
            "Epoch 39/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.1124e-06\n",
            "Epoch 40/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 3.9780e-06\n",
            "Epoch 41/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.5880e-06\n",
            "Epoch 42/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 1.6849e-06\n",
            "Epoch 43/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.0958e-06\n",
            "Epoch 44/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 7.1237e-07\n",
            "Epoch 45/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.6285e-07\n",
            "Epoch 46/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.0121e-07\n",
            "Epoch 47/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1.9591e-07\n",
            "Epoch 48/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1.2754e-07\n",
            "Epoch 49/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 8.3172e-08\n",
            "Epoch 50/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 5.4339e-08\n",
            "Epoch 51/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 3.5351e-08\n",
            "Epoch 52/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 2.2900e-08\n",
            "Epoch 53/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.4925e-08\n",
            "Epoch 54/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 9.7375e-09\n",
            "Epoch 55/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.3688e-09\n",
            "Epoch 56/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.1993e-09\n",
            "Epoch 57/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 2.7632e-09\n",
            "Epoch 58/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1.7885e-09\n",
            "Epoch 59/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1.1449e-09\n",
            "Epoch 60/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 7.4829e-10\n",
            "Epoch 61/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.6434e-10\n",
            "Epoch 62/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 3.2028e-10\n",
            "Epoch 63/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.0023e-10\n",
            "Epoch 64/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.2571e-10\n",
            "Epoch 65/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 8.8266e-11\n",
            "Epoch 66/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 5.6616e-11\n",
            "Epoch 67/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 3.1878e-11\n",
            "Epoch 68/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.1942e-11\n",
            "Epoch 69/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.4666e-11\n",
            "Epoch 70/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 7.9353e-12\n",
            "Epoch 71/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 4.0473e-12\n",
            "Epoch 72/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 3.9336e-12\n",
            "Epoch 73/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 3.6380e-12\n",
            "Epoch 74/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 3.9336e-12\n",
            "Epoch 75/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 3.6380e-12\n",
            "Epoch 76/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 3.9336e-12\n",
            "Epoch 77/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 3.6380e-12\n",
            "Epoch 78/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 3.9336e-12\n",
            "Epoch 79/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 3.6380e-12\n",
            "Epoch 80/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 3.9336e-12\n",
            "Epoch 81/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 3.6380e-12\n",
            "Epoch 82/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 3.9336e-12\n",
            "Epoch 83/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 3.6380e-12\n",
            "Epoch 84/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.9336e-12\n",
            "Epoch 85/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 3.6380e-12\n",
            "Epoch 86/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 3.9336e-12\n",
            "Epoch 87/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 3.6380e-12\n",
            "Epoch 88/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 3.9336e-12\n",
            "Epoch 89/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 3.6380e-12\n",
            "Epoch 90/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.9336e-12\n",
            "Epoch 91/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.6380e-12\n",
            "Epoch 92/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 3.9336e-12\n",
            "Epoch 93/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.6380e-12\n",
            "Epoch 94/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.9336e-12\n",
            "Epoch 95/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.6380e-12\n",
            "Epoch 96/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 3.9336e-12\n",
            "Epoch 97/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.6380e-12\n",
            "Epoch 98/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 3.9336e-12\n",
            "Epoch 99/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 3.6380e-12\n",
            "Epoch 100/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.9336e-12\n",
            "[[ 6.999998]\n",
            " [18.999998]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A2"
      ],
      "metadata": {
        "id": "gX-ss4YMBIJl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================================================\n",
        "# PROBLEM A2\n",
        "#\n",
        "# Build a Neural Network Model for Horse or Human Dataset.\n",
        "# The test will expect it to classify binary classes.\n",
        "# Your input layer should accept 150x150 with 3 bytes color as the input shape.\n",
        "# Don't use lambda layers in your model.\n",
        "#\n",
        "# The dataset used in this problem is created by Laurence Moroney (laurencemoroney.com).\n",
        "#\n",
        "# Desired accuracy and validation_accuracy > 83%\n",
        "# ======================================================================================\n",
        "\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from keras_preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "\n",
        "def solution_A2():\n",
        "    data_url_1 = 'https://github.com/dicodingacademy/assets/releases/download/release-horse-or-human/horse-or-human.zip'\n",
        "    urllib.request.urlretrieve(data_url_1, 'horse-or-human-a2.zip')\n",
        "    local_file = 'horse-or-human-a2.zip'\n",
        "    zip_ref = zipfile.ZipFile(local_file, 'r')\n",
        "    zip_ref.extractall('data/horse-or-human-a2')\n",
        "\n",
        "    data_url_2 = 'https://github.com/dicodingacademy/assets/raw/main/Simulation/machine_learning/validation-horse-or-human.zip'\n",
        "    urllib.request.urlretrieve(data_url_2, 'validation-horse-or-human-a2.zip')\n",
        "    local_file = 'validation-horse-or-human-a2.zip'\n",
        "    zip_ref = zipfile.ZipFile(local_file, 'r')\n",
        "    zip_ref.extractall('data/validation-horse-or-human-a2')\n",
        "    zip_ref.close()\n",
        "\n",
        "    TRAINING_DIR = 'data/horse-or-human-a2'\n",
        "    VALID_DIR = 'data/validation-horse-or-human-a2'\n",
        "    train_datagen = ImageDataGenerator(\n",
        "        rescale = 1/255,\n",
        "        # rotation_range=10,\n",
        "        # height_shift_range=0.1,\n",
        "        # width_shift_range=0.1,\n",
        "        # zoom_range=0.1,\n",
        "        # shear_range=0.1,\n",
        "        # horizontal_flip=True,\n",
        "    )\n",
        "\n",
        "    valid_datagen = ImageDataGenerator(\n",
        "        rescale = 1/255,\n",
        "    )\n",
        "\n",
        "    TARGET_SIZE = (150, 150)\n",
        "    BATCH_SIZE = 32\n",
        "    SEED=42\n",
        "\n",
        "    # YOUR IMAGE SIZE SHOULD BE 150x150\n",
        "    train_generator=train_datagen.flow_from_directory(\n",
        "        directory=TRAINING_DIR,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        target_size=TARGET_SIZE,\n",
        "        class_mode='binary',\n",
        "        seed=SEED,\n",
        "    )\n",
        "    \n",
        "    valid_generator=valid_datagen.flow_from_directory(\n",
        "        directory=VALID_DIR,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        target_size=TARGET_SIZE,\n",
        "        class_mode='binary',\n",
        "        seed=SEED,\n",
        "    )\n",
        "\n",
        "    INPUT_SHAPE = TARGET_SIZE + (3,)\n",
        "    model=tf.keras.models.Sequential([\n",
        "        # YOUR CODE HERE, end with a Neuron Dense, activated by sigmoid\n",
        "        tf.keras.layers.Conv2D(\n",
        "            filters=16,\n",
        "            input_shape=INPUT_SHAPE,\n",
        "            kernel_size=(3, 3),\n",
        "            activation=tf.nn.relu,\n",
        "            padding='same',\n",
        "            kernel_initializer=tf.keras.initializers.he_normal,\n",
        "        ),\n",
        "        tf.keras.layers.MaxPool2D(\n",
        "            pool_size=(2, 2),\n",
        "            padding='same',\n",
        "        ),\n",
        "        tf.keras.layers.Conv2D(\n",
        "            filters=32,\n",
        "            kernel_size=(3, 3),\n",
        "            activation=tf.nn.relu,\n",
        "            padding='same',\n",
        "            kernel_initializer=tf.keras.initializers.he_normal,\n",
        "        ),\n",
        "        tf.keras.layers.Conv2D(\n",
        "            filters=32,\n",
        "            kernel_size=(3, 3),\n",
        "            activation=tf.nn.relu,\n",
        "            padding='same',\n",
        "            kernel_initializer=tf.keras.initializers.he_normal,\n",
        "        ),\n",
        "        tf.keras.layers.MaxPool2D(\n",
        "            pool_size=(2, 2),\n",
        "            padding='same',\n",
        "        ),\n",
        "        tf.keras.layers.Conv2D(\n",
        "            filters=64,\n",
        "            kernel_size=(3, 3),\n",
        "            activation=tf.nn.relu,\n",
        "            padding='same',\n",
        "            kernel_initializer=tf.keras.initializers.he_normal,\n",
        "        ),\n",
        "        tf.keras.layers.Conv2D(\n",
        "            filters=64,\n",
        "            kernel_size=(3, 3),\n",
        "            activation=tf.nn.relu,\n",
        "            padding='same',\n",
        "            kernel_initializer=tf.keras.initializers.he_normal,\n",
        "        ),\n",
        "        tf.keras.layers.MaxPool2D(\n",
        "            pool_size=(2, 2),\n",
        "            padding='same',\n",
        "        ),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(\n",
        "            units=1024, \n",
        "            activation=tf.nn.relu, \n",
        "            kernel_initializer=tf.keras.initializers.he_normal,\n",
        "        ),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.Dense(\n",
        "            units=1024, \n",
        "            activation=tf.nn.relu, \n",
        "            kernel_initializer=tf.keras.initializers.he_normal,\n",
        "        ),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "\n",
        "    model.compile(\n",
        "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "        optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.001),\n",
        "        metrics = [tf.keras.metrics.BinaryAccuracy()]\n",
        "    )\n",
        "\n",
        "    callbacks=[\n",
        "              tf.keras.callbacks.ModelCheckpoint(\n",
        "                  filepath='model_a2.h5',\n",
        "                  monitor='val_binary_accuracy',\n",
        "                  mode='max',\n",
        "                  save_best_only=True,\n",
        "                  verbose=2,\n",
        "              ),\n",
        "              tf.keras.callbacks.EarlyStopping(\n",
        "                  monitor='val_loss',\n",
        "                  patience=5,\n",
        "              ),\n",
        "    ]\n",
        "\n",
        "    model.fit(\n",
        "        train_generator,\n",
        "        epochs=20,\n",
        "        validation_data=valid_generator,\n",
        "        callbacks=callbacks\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# The code below is to save your model as a .h5 file.\n",
        "# It will be saved automatically in your Submission folder.\n",
        "if __name__ == '__main__':\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    model=solution_A2()\n",
        "    model.save(\"model_A2.h5\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBRUv0OLCp95",
        "outputId": "4f3179ca-b504-4793-bbbb-b0ddd2be3a19"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1027 images belonging to 2 classes.\n",
            "Found 256 images belonging to 2 classes.\n",
            "Epoch 1/20\n",
            "33/33 [==============================] - ETA: 0s - loss: 2.2287 - binary_accuracy: 0.6981\n",
            "Epoch 1: val_binary_accuracy improved from -inf to 0.74609, saving model to model_a2.h5\n",
            "33/33 [==============================] - 8s 223ms/step - loss: 2.2287 - binary_accuracy: 0.6981 - val_loss: 0.6036 - val_binary_accuracy: 0.7461\n",
            "Epoch 2/20\n",
            "33/33 [==============================] - ETA: 0s - loss: 0.2344 - binary_accuracy: 0.8987\n",
            "Epoch 2: val_binary_accuracy did not improve from 0.74609\n",
            "33/33 [==============================] - 7s 204ms/step - loss: 0.2344 - binary_accuracy: 0.8987 - val_loss: 5.4587 - val_binary_accuracy: 0.5000\n",
            "Epoch 3/20\n",
            "33/33 [==============================] - ETA: 0s - loss: 0.5916 - binary_accuracy: 0.9026\n",
            "Epoch 3: val_binary_accuracy improved from 0.74609 to 0.82031, saving model to model_a2.h5\n",
            "33/33 [==============================] - 7s 214ms/step - loss: 0.5916 - binary_accuracy: 0.9026 - val_loss: 2.5990 - val_binary_accuracy: 0.8203\n",
            "Epoch 4/20\n",
            "33/33 [==============================] - ETA: 0s - loss: 1.3795 - binary_accuracy: 0.9484\n",
            "Epoch 4: val_binary_accuracy did not improve from 0.82031\n",
            "33/33 [==============================] - 7s 203ms/step - loss: 1.3795 - binary_accuracy: 0.9484 - val_loss: 0.8142 - val_binary_accuracy: 0.7891\n",
            "Epoch 5/20\n",
            "33/33 [==============================] - ETA: 0s - loss: 0.1448 - binary_accuracy: 0.9503\n",
            "Epoch 5: val_binary_accuracy improved from 0.82031 to 0.88281, saving model to model_a2.h5\n",
            "33/33 [==============================] - 7s 213ms/step - loss: 0.1448 - binary_accuracy: 0.9503 - val_loss: 1.2970 - val_binary_accuracy: 0.8828\n",
            "Epoch 6/20\n",
            "33/33 [==============================] - ETA: 0s - loss: 0.6835 - binary_accuracy: 0.9513\n",
            "Epoch 6: val_binary_accuracy did not improve from 0.88281\n",
            "33/33 [==============================] - 7s 204ms/step - loss: 0.6835 - binary_accuracy: 0.9513 - val_loss: 1.6500 - val_binary_accuracy: 0.8594\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A3"
      ],
      "metadata": {
        "id": "aG18zjiDBJQc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================================================================\n",
        "# PROBLEM A3\n",
        "#\n",
        "# Build a classifier for the Human or Horse Dataset with Transfer Learning.\n",
        "# The test will expect it to classify binary classes.\n",
        "# Note that all the layers in the pre-trained model are non-trainable.\n",
        "# Do not use lambda layers in your model.\n",
        "#\n",
        "# The horse-or-human dataset used in this problem is created by Laurence Moroney (laurencemoroney.com).\n",
        "# Inception_v3, pre-trained model used in this problem is developed by Google.\n",
        "#\n",
        "# Desired accuracy and validation_accuracy > 97%.\n",
        "# =======================================================================================================\n",
        "\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import tensorflow as tf\n",
        "from keras_preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "\n",
        "\n",
        "def solution_A3():\n",
        "    TARGET_SHAPE = (150, 150)\n",
        "    INPUT_SHAPE = (150, 150) + (3,)\n",
        "    inceptionv3 = 'https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
        "    urllib.request.urlretrieve(\n",
        "        inceptionv3, 'inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5')\n",
        "    local_weights_file = 'inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
        "\n",
        "    pre_trained_model =  tf.keras.applications.inception_v3.InceptionV3(\n",
        "        include_top = False,\n",
        "        weights=None,\n",
        "        input_shape = INPUT_SHAPE\n",
        "    )\n",
        "\n",
        "    pre_trained_model.load_weights(local_weights_file)\n",
        "\n",
        "    for layer in pre_trained_model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    last_layer =  pre_trained_model.get_layer('mixed8')\n",
        "    last_output = last_layer.output\n",
        "\n",
        "    data_url_1 = 'https://github.com/dicodingacademy/assets/releases/download/release-horse-or-human/horse-or-human.zip'\n",
        "    urllib.request.urlretrieve(data_url_1, 'horse-or-human-a3.zip')\n",
        "    local_file = 'horse-or-human-a3.zip'\n",
        "    zip_ref = zipfile.ZipFile(local_file, 'r')\n",
        "    zip_ref.extractall('data/horse-or-human-a3')\n",
        "    zip_ref.close()\n",
        "\n",
        "    data_url_2 = 'https://github.com/dicodingacademy/assets/raw/main/Simulation/machine_learning/validation-horse-or-human.zip'\n",
        "    urllib.request.urlretrieve(data_url_2, 'validation-horse-or-human-a3.zip')\n",
        "    local_file = 'validation-horse-or-human-a3.zip'\n",
        "    zip_ref = zipfile.ZipFile(local_file, 'r')\n",
        "    zip_ref.extractall('data/validation-horse-or-human-a3')\n",
        "    zip_ref.close()\n",
        "\n",
        "    train_dir = 'data/horse-or-human-a3'\n",
        "    validation_dir = 'data/validation-horse-or-human-a3'\n",
        "\n",
        "    train_datagen = ImageDataGenerator(\n",
        "        rescale=1. / 255,\n",
        "        rotation_range=40,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest'\n",
        "    )\n",
        "\n",
        "    validation_datagen = ImageDataGenerator(\n",
        "        rescale=1./255\n",
        "    )\n",
        "\n",
        "    train_generator = train_datagen.flow_from_directory(\n",
        "        directory=train_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=20,\n",
        "        class_mode='binary',\n",
        "        shuffle=True,\n",
        "        seed=42,\n",
        "    )\n",
        "\n",
        "    validation_generator = validation_datagen.flow_from_directory(\n",
        "        directory=validation_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=20,\n",
        "        class_mode='binary',\n",
        "        shuffle=True,\n",
        "        seed=42\n",
        "    )\n",
        "\n",
        "\n",
        "    x = tf.keras.layers.Flatten()(last_output)\n",
        "    x = tf.keras.layers.Dense(\n",
        "        units=1024, \n",
        "        activation=tf.nn.relu, \n",
        "        kernel_initializer=tf.keras.initializers.he_normal,\n",
        "        )(x)\n",
        "    x = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = Model(pre_trained_model.input, x)\n",
        "\n",
        "    model.compile(optimizer=RMSprop(learning_rate=0.0001),\n",
        "                  loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "                  metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
        "    \n",
        "    model.fit(\n",
        "        train_generator,\n",
        "        epochs=10,\n",
        "        validation_data=validation_generator,\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# The code below is to save your model as a .h5 file.\n",
        "# It will be saved automatically in your Submission folder.\n",
        "if __name__ == '__main__':\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    model=solution_A3()\n",
        "    model.save(\"model_A3.h5\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nNSB2pyfHgpU",
        "outputId": "aab13ac1-6979-48cf-d0b0-ae47e7794d13"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1027 images belonging to 2 classes.\n",
            "Found 256 images belonging to 2 classes.\n",
            "Epoch 1/10\n",
            "52/52 [==============================] - 21s 339ms/step - loss: 0.1785 - binary_accuracy: 0.9435 - val_loss: 0.2862 - val_binary_accuracy: 0.8555\n",
            "Epoch 2/10\n",
            "52/52 [==============================] - 13s 242ms/step - loss: 0.0562 - binary_accuracy: 0.9796 - val_loss: 0.0069 - val_binary_accuracy: 0.9961\n",
            "Epoch 3/10\n",
            "52/52 [==============================] - 12s 225ms/step - loss: 0.0240 - binary_accuracy: 0.9932 - val_loss: 0.0134 - val_binary_accuracy: 0.9961\n",
            "Epoch 4/10\n",
            "52/52 [==============================] - 12s 222ms/step - loss: 0.0318 - binary_accuracy: 0.9864 - val_loss: 5.5418e-04 - val_binary_accuracy: 1.0000\n",
            "Epoch 5/10\n",
            "52/52 [==============================] - 12s 224ms/step - loss: 0.0169 - binary_accuracy: 0.9932 - val_loss: 0.0027 - val_binary_accuracy: 1.0000\n",
            "Epoch 6/10\n",
            "52/52 [==============================] - 12s 221ms/step - loss: 0.0135 - binary_accuracy: 0.9951 - val_loss: 0.0198 - val_binary_accuracy: 0.9883\n",
            "Epoch 7/10\n",
            "52/52 [==============================] - 12s 221ms/step - loss: 0.0068 - binary_accuracy: 0.9971 - val_loss: 0.0050 - val_binary_accuracy: 1.0000\n",
            "Epoch 8/10\n",
            "52/52 [==============================] - 13s 258ms/step - loss: 0.0340 - binary_accuracy: 0.9932 - val_loss: 0.0146 - val_binary_accuracy: 0.9961\n",
            "Epoch 9/10\n",
            "52/52 [==============================] - 12s 227ms/step - loss: 0.0090 - binary_accuracy: 0.9961 - val_loss: 0.0036 - val_binary_accuracy: 0.9961\n",
            "Epoch 10/10\n",
            "52/52 [==============================] - 12s 237ms/step - loss: 0.0079 - binary_accuracy: 0.9961 - val_loss: 5.6284e-06 - val_binary_accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A4"
      ],
      "metadata": {
        "id": "tkK7tPpWBJTo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================================================================\n",
        "# PROBLEM A4\n",
        "#\n",
        "# Build and train a binary classifier for the IMDB review dataset.\n",
        "# The classifier should have a final layer with 1 neuron activated by sigmoid.\n",
        "# Do not use lambda layers in your model.\n",
        "#\n",
        "# The dataset used in this problem is originally published in http://ai.stanford.edu/~amaas/data/sentiment/\n",
        "#\n",
        "# Desired accuracy and validation_accuracy > 83%\n",
        "# ===========================================================================================================\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "def solution_A4():\n",
        "    imdb, info = tfds.load(\"imdb_reviews\", with_info=True, as_supervised=True)\n",
        "    # YOUR CODE HERE\n",
        "    train_data = imdb['train']\n",
        "    test_data = imdb['test']\n",
        "\n",
        "    training_sentences = []\n",
        "    training_labels = []\n",
        "    testing_sentences = []\n",
        "    testing_labels = []\n",
        "\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    for s, l in train_data:\n",
        "        training_sentences.append(s.numpy().decode('utf8'))\n",
        "        training_labels.append(l.numpy())\n",
        "\n",
        "    for s, l in test_data:\n",
        "        testing_sentences.append(s.numpy().decode('utf8'))\n",
        "        testing_labels.append(l.numpy())\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    training_labels = np.array(training_labels)\n",
        "    testing_labels = np.array(testing_labels)\n",
        "\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    # Make sure you used all of these parameters or test may fail\n",
        "    vocab_size = 10000\n",
        "    embedding_dim = 16\n",
        "    max_length = 120\n",
        "    trunc_type = 'post'\n",
        "    oov_tok = \"<OOV>\"\n",
        "\n",
        "    # Fit your tokenizer with training data\n",
        "    tokenizer =  tf.keras.preprocessing.text.Tokenizer(\n",
        "        num_words=vocab_size,\n",
        "        oov_token=oov_tok,\n",
        "    )\n",
        "\n",
        "    tokenizer.fit_on_texts(training_sentences)\n",
        "    train_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
        "    train_padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "        train_sequences,\n",
        "        maxlen=max_length,\n",
        "        truncating=trunc_type,\n",
        "    )\n",
        "\n",
        "    testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
        "    testing_padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "        testing_sequences,\n",
        "        maxlen=max_length,\n",
        "        truncating=trunc_type,\n",
        "    )\n",
        "\n",
        "\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(\n",
        "            input_dim=vocab_size,\n",
        "            input_length=max_length,\n",
        "            output_dim=embedding_dim,\n",
        "        ),\n",
        "        tf.keras.layers.Bidirectional(\n",
        "            tf.keras.layers.LSTM(32, return_sequences=True)\n",
        "            ),\n",
        "        # tf.keras.layers.Bidirectional(\n",
        "        #     tf.keras.layers.LSTM(32, return_sequences=True)\n",
        "        #     ),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(),\n",
        "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "        metrics=[tf.keras.metrics.BinaryAccuracy()]\n",
        "    )\n",
        "\n",
        "    model.fit(\n",
        "        x=train_padded_sequences,\n",
        "        y=training_labels,\n",
        "        batch_size=32,\n",
        "        epochs=20,\n",
        "        validation_data=(testing_padded_sequences, testing_labels),\n",
        "        validation_batch_size=32,\n",
        "    )\n",
        "\n",
        "    # return model\n",
        "\n",
        "\n",
        "# The code below is to save your model as a .h5 file.\n",
        "# It will be saved automatically in your Submission folder.\n",
        "if __name__ == '__main__':\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    model = solution_A4()\n",
        "    model.save(\"model_A4.h5\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "pRzEZjUASYTv",
        "outputId": "d292a1a0-069a-478c-9a34-d89a8d01b40d"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-5833541f306f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;31m# DO NOT CHANGE THIS CODE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msolution_A4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m     \u001b[0;31m# model.save(\"model_A4.h5\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-50-5833541f306f>\u001b[0m in \u001b[0;36msolution_A4\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mtesting_sentences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mtesting_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1221\u001b[0m     \"\"\"\n\u001b[1;32m   1222\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1223\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1224\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1187\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1189\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1190\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A5"
      ],
      "metadata": {
        "id": "K8qltAESBJWm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =======================================================================================\n",
        "# PROBLEM A5\n",
        "#\n",
        "# Build and train a neural network model using the Sunspots.csv dataset.\n",
        "# Use MAE as the metrics of your neural network model.\n",
        "# We provided code for normalizing the data. Please do not change the code.\n",
        "# Do not use lambda layers in your model.\n",
        "#\n",
        "# The dataset used in this problem is downloaded from kaggle.com/robervalt/sunspots\n",
        "#\n",
        "# Desired MAE < 0.15 on the normalized dataset.\n",
        "# ========================================================================================\n",
        "\n",
        "import csv\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import urllib\n",
        "\n",
        "# DO NOT CHANGE THIS CODE\n",
        "def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n",
        "    series = tf.expand_dims(series, axis=-1)\n",
        "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)\n",
        "    ds = ds.flat_map(lambda w: w.batch(window_size + 1))\n",
        "    ds = ds.shuffle(shuffle_buffer)\n",
        "    ds = ds.map(lambda w: (w[:-1], w[1:]))\n",
        "    return ds.batch(batch_size).prefetch(1)\n",
        "\n",
        "\n",
        "def solution_A5():\n",
        "    data_url = 'https://github.com/dicodingacademy/assets/raw/main/Simulation/machine_learning/sunspots.csv'\n",
        "    urllib.request.urlretrieve(data_url, 'sunspots.csv')\n",
        "\n",
        "    time_step = []\n",
        "    sunspots = []\n",
        "\n",
        "    with open('sunspots.csv') as csvfile:\n",
        "      reader = csv.reader(csvfile, delimiter=',')\n",
        "      next(reader)\n",
        "      for row in reader:\n",
        "        sunspots.append(float(row[2]))\n",
        "        time_step.append(int(row[0]))\n",
        "\n",
        "    series=np.array(sunspots)\n",
        "\n",
        "    # Normalization Function. DO NOT CHANGE THIS CODE\n",
        "    min=np.min(series)\n",
        "    max=np.max(series)\n",
        "    series -= min\n",
        "    series /= max\n",
        "    time=np.array(time_step)\n",
        "\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    split_time=3000\n",
        "\n",
        "\n",
        "    time_train= time[:split_time]\n",
        "    x_train=  series[:split_time]\n",
        "    time_valid= time[split_time:]\n",
        "    x_valid=  series[split_time:]\n",
        "\n",
        "\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    window_size=30\n",
        "    batch_size=32\n",
        "    shuffle_buffer_size=1000\n",
        "\n",
        "\n",
        "    train_set=windowed_dataset(x_train, window_size=window_size,\n",
        "                               batch_size=batch_size, shuffle_buffer=shuffle_buffer_size)\n",
        "\n",
        "\n",
        "    model=tf.keras.models.Sequential([\n",
        "      # YOUR CODE HERE.\n",
        "      tf.keras.layers.Dense(units=20, input_shape=[window_size], activation=tf.nn.relu,\n",
        "                            kernel_initializer=tf.keras.initializers.he_normal),\n",
        "      tf.keras.layers.Dense(units=10, activation=tf.nn.relu,\n",
        "                            kernel_initializer=tf.keras.initializers.he_normal),\n",
        "      tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "    # YOUR CODE\n",
        "    model.compile(\n",
        "        loss=tf.keras.losses.MeanAbsoluteError(),\n",
        "        optimizer=tf.keras.optimizers.SGD(learning_rate=1e-4, momentum=0.5)\n",
        "    )\n",
        "    model.fit(\n",
        "        train_set,\n",
        "        epochs=100\n",
        "    )\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "# The code below is to save your model as a .h5 file.\n",
        "# It will be saved automatically in your Submission folder.\n",
        "if __name__ == '__main__':\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    model=solution_A5()\n",
        "    model.save(\"model_A5.h5\")\n"
      ],
      "metadata": {
        "id": "RlqnEKYlBJY-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39e17f68-500f-4daa-8fc3-d1a03841fa89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "93/93 [==============================] - 1s 5ms/step - loss: 0.2845\n",
            "Epoch 2/100\n",
            "93/93 [==============================] - 1s 4ms/step - loss: 0.2098\n",
            "Epoch 3/100\n",
            "93/93 [==============================] - 1s 5ms/step - loss: 0.1619\n",
            "Epoch 4/100\n",
            "93/93 [==============================] - 1s 4ms/step - loss: 0.1331\n",
            "Epoch 5/100\n",
            "93/93 [==============================] - 1s 4ms/step - loss: 0.1173\n",
            "Epoch 6/100\n",
            "93/93 [==============================] - 1s 5ms/step - loss: 0.1086\n",
            "Epoch 7/100\n",
            "93/93 [==============================] - 1s 4ms/step - loss: 0.1032\n",
            "Epoch 8/100\n",
            "93/93 [==============================] - 1s 4ms/step - loss: 0.0993\n",
            "Epoch 9/100\n",
            "93/93 [==============================] - 1s 5ms/step - loss: 0.0962\n",
            "Epoch 10/100\n",
            "93/93 [==============================] - 1s 5ms/step - loss: 0.0934\n",
            "Epoch 11/100\n",
            "93/93 [==============================] - 1s 5ms/step - loss: 0.0910\n",
            "Epoch 12/100\n",
            "93/93 [==============================] - 1s 5ms/step - loss: 0.0888\n",
            "Epoch 13/100\n",
            "93/93 [==============================] - 1s 5ms/step - loss: 0.0870\n",
            "Epoch 14/100\n",
            "93/93 [==============================] - 1s 4ms/step - loss: 0.0854\n",
            "Epoch 15/100\n",
            "93/93 [==============================] - 1s 4ms/step - loss: 0.0840\n",
            "Epoch 16/100\n",
            "93/93 [==============================] - 1s 5ms/step - loss: 0.0828\n",
            "Epoch 17/100\n",
            "93/93 [==============================] - 1s 4ms/step - loss: 0.0817\n",
            "Epoch 18/100\n",
            "93/93 [==============================] - 1s 5ms/step - loss: 0.0809\n",
            "Epoch 19/100\n",
            "93/93 [==============================] - 1s 5ms/step - loss: 0.0802\n",
            "Epoch 20/100\n",
            "93/93 [==============================] - 1s 5ms/step - loss: 0.0796\n",
            "Epoch 21/100\n",
            "93/93 [==============================] - 1s 4ms/step - loss: 0.0791\n",
            "Epoch 22/100\n",
            "93/93 [==============================] - 1s 5ms/step - loss: 0.0787\n",
            "Epoch 23/100\n",
            "93/93 [==============================] - 1s 8ms/step - loss: 0.0783\n",
            "Epoch 24/100\n",
            "93/93 [==============================] - 1s 8ms/step - loss: 0.0780\n",
            "Epoch 25/100\n",
            "93/93 [==============================] - 1s 4ms/step - loss: 0.0777\n",
            "Epoch 26/100\n",
            "93/93 [==============================] - 1s 4ms/step - loss: 0.0775\n",
            "Epoch 27/100\n",
            "93/93 [==============================] - 1s 5ms/step - loss: 0.0773\n",
            "Epoch 28/100\n",
            "93/93 [==============================] - 1s 5ms/step - loss: 0.0771\n",
            "Epoch 29/100\n",
            "93/93 [==============================] - 1s 5ms/step - loss: 0.0769\n",
            "Epoch 30/100\n",
            "93/93 [==============================] - 1s 5ms/step - loss: 0.0768\n",
            "Epoch 31/100\n",
            "93/93 [==============================] - 1s 5ms/step - loss: 0.0766\n",
            "Epoch 32/100\n",
            "93/93 [==============================] - 1s 5ms/step - loss: 0.0765\n",
            "Epoch 33/100\n",
            "93/93 [==============================] - 1s 5ms/step - loss: 0.0764\n",
            "Epoch 34/100\n",
            "93/93 [==============================] - 1s 5ms/step - loss: 0.0762\n",
            "Epoch 35/100\n",
            "93/93 [==============================] - 1s 5ms/step - loss: 0.0761\n",
            "Epoch 36/100\n",
            "93/93 [==============================] - 1s 5ms/step - loss: 0.0760\n",
            "Epoch 37/100\n",
            "93/93 [==============================] - 1s 5ms/step - loss: 0.0759\n",
            "Epoch 38/100\n",
            "93/93 [==============================] - 1s 5ms/step - loss: 0.0758\n",
            "Epoch 39/100\n",
            "93/93 [==============================] - 1s 5ms/step - loss: 0.0757\n",
            "Epoch 40/100\n",
            "93/93 [==============================] - 1s 5ms/step - loss: 0.0756\n",
            "Epoch 41/100\n",
            "93/93 [==============================] - 1s 5ms/step - loss: 0.0755\n",
            "Epoch 42/100\n",
            "93/93 [==============================] - 1s 4ms/step - loss: 0.0754\n",
            "Epoch 43/100\n",
            "93/93 [==============================] - 1s 5ms/step - loss: 0.0754\n",
            "Epoch 44/100\n",
            "93/93 [==============================] - 1s 4ms/step - loss: 0.0753\n",
            "Epoch 45/100\n",
            "93/93 [==============================] - 1s 4ms/step - loss: 0.0752\n",
            "Epoch 46/100\n",
            "93/93 [==============================] - 1s 4ms/step - loss: 0.0751\n",
            "Epoch 47/100\n",
            "93/93 [==============================] - 1s 5ms/step - loss: 0.0750\n",
            "Epoch 48/100\n",
            "93/93 [==============================] - 1s 4ms/step - loss: 0.0750\n",
            "Epoch 49/100\n",
            "93/93 [==============================] - 1s 4ms/step - loss: 0.0749\n",
            "Epoch 50/100\n",
            "93/93 [==============================] - 1s 5ms/step - loss: 0.0748\n",
            "Epoch 51/100\n",
            "93/93 [==============================] - 1s 4ms/step - loss: 0.0747\n",
            "Epoch 52/100\n",
            "93/93 [==============================] - 1s 5ms/step - loss: 0.0747\n",
            "Epoch 53/100\n",
            "93/93 [==============================] - 1s 4ms/step - loss: 0.0746\n",
            "Epoch 54/100\n",
            "93/93 [==============================] - 1s 4ms/step - loss: 0.0745\n",
            "Epoch 55/100\n",
            "93/93 [==============================] - 1s 5ms/step - loss: 0.0745\n",
            "Epoch 56/100\n",
            "93/93 [==============================] - 1s 4ms/step - loss: 0.0744\n",
            "Epoch 57/100\n",
            "93/93 [==============================] - 1s 5ms/step - loss: 0.0743\n",
            "Epoch 58/100\n",
            "93/93 [==============================] - 1s 5ms/step - loss: 0.0743\n",
            "Epoch 59/100\n",
            "93/93 [==============================] - 1s 4ms/step - loss: 0.0742\n",
            "Epoch 60/100\n",
            "93/93 [==============================] - 1s 5ms/step - loss: 0.0741\n",
            "Epoch 61/100\n",
            "93/93 [==============================] - 1s 4ms/step - loss: 0.0741\n",
            "Epoch 62/100\n",
            "93/93 [==============================] - 1s 4ms/step - loss: 0.0740\n",
            "Epoch 63/100\n",
            "93/93 [==============================] - 1s 5ms/step - loss: 0.0740\n",
            "Epoch 64/100\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 0.0739\n",
            "Epoch 65/100\n",
            "93/93 [==============================] - 1s 12ms/step - loss: 0.0739\n",
            "Epoch 66/100\n",
            "93/93 [==============================] - 1s 5ms/step - loss: 0.0738\n",
            "Epoch 67/100\n",
            "93/93 [==============================] - 1s 4ms/step - loss: 0.0737\n",
            "Epoch 68/100\n",
            "93/93 [==============================] - 1s 4ms/step - loss: 0.0737\n",
            "Epoch 69/100\n",
            "93/93 [==============================] - 1s 5ms/step - loss: 0.0736\n",
            "Epoch 70/100\n",
            "93/93 [==============================] - 1s 4ms/step - loss: 0.0736\n",
            "Epoch 71/100\n",
            "93/93 [==============================] - 1s 4ms/step - loss: 0.0735\n",
            "Epoch 72/100\n",
            "93/93 [==============================] - 1s 5ms/step - loss: 0.0735\n",
            "Epoch 73/100\n",
            "93/93 [==============================] - 1s 4ms/step - loss: 0.0734\n",
            "Epoch 74/100\n",
            "93/93 [==============================] - 1s 5ms/step - loss: 0.0734\n",
            "Epoch 75/100\n",
            "93/93 [==============================] - 1s 5ms/step - loss: 0.0733\n",
            "Epoch 76/100\n",
            "93/93 [==============================] - 1s 4ms/step - loss: 0.0733\n",
            "Epoch 77/100\n",
            "93/93 [==============================] - 1s 5ms/step - loss: 0.0732\n",
            "Epoch 78/100\n",
            "93/93 [==============================] - 1s 5ms/step - loss: 0.0732\n",
            "Epoch 79/100\n"
          ]
        }
      ]
    }
  ]
}